# intelligent-agent-rag-llm

# ğŸ¤– AGENTE-INTELIGENTE  
## ğŸš€ FastAPI + LangChain + RAG + MLflow + Hugging Face


---

## ğŸ… Badges

- ğŸ“¦ Tamanho do repositÃ³rio:  
  ![GitHub repo size](https://img.shields.io/github/repo-size/Rogerio5/AGENTE-INTELIGENTE)

- ğŸ“„ LicenÃ§a do projeto:  
  ![GitHub license](https://img.shields.io/github/license/Rogerio5/AGENTE-INTELIGENTE)

---

## ğŸ“‹ Ãndice / Table of Contents

- [ğŸ“– DescriÃ§Ã£o / Description](#-descriÃ§Ã£o--description)  
- [ğŸ“Œ Status do Projeto / Project-Status](#-status-do-projeto--project-status)  
- [âš™ï¸ Arquitetura / Architecture](#-arquitetura--architecture)  
- [ğŸš€ Guia de InstalaÃ§Ã£o / Installation Guide](#-guia-de-instalaÃ§Ã£o--installation-guide)  
- [ğŸ§ª Testes / Tests](#-testes--tests)  
- [ğŸ§° Tecnologias / Technologies](#-tecnologias--technologies)  
- [ğŸ‘¨â€ğŸ’» Desenvolvedor / Developer](#-desenvolvedor--developer)  
- [ğŸ“œ LicenÃ§a / License](#-licenÃ§a--license)  
- [ğŸ ConclusÃ£o / Conclusion](#-conclusÃ£o--conclusion)

---

## ğŸ“– DescriÃ§Ã£o / Description

Este projeto Ã© um agente inteligente que utiliza **LLMs** e tÃ©cnicas de **RAG (Retrieval-Augmented Generation)** para responder perguntas com contexto.  
A arquitetura combina **FastAPI**, **LangChain**, **MLflow**, **Docker** e **Hugging Face**, com foco em automaÃ§Ã£o, rastreabilidade e escalabilidade.

---

## ğŸ“Œ Status do Projeto / Project Status

![Status](https://img.shields.io/badge/Status-Em%20Desenvolvimento-yellow?style=for-the-badge)

---

## âš™ï¸ Arquitetura / Architecture

- API FastAPI expÃµe os endpoints: `/ask`, `/train`, `/metrics`  
- OrquestraÃ§Ã£o com LangChain e RAG (FAISS + embeddings)  
- LLM via Hugging Face (`gpt2` como modelo base)  
- MLOps com MLflow para tracking de runs, mÃ©tricas e artefatos  
- Deploy via Dockerfile e docker-compose com serviÃ§o MLflow  
- Futuro: Prometheus/Grafana + Kubernetes (`infra/k8s`)  

### ğŸ” Fluxo de execuÃ§Ã£o

1. UsuÃ¡rio chama `/ask`  
2. Agente consulta RAG e monta prompt com contexto  
3. LLM gera resposta  
4. Logging de inferÃªncia no MLflow  
5. MÃ©tricas expostas em `/metrics`

---

## ğŸš€ Guia de InstalaÃ§Ã£o / Installation Guide

### âœ… PrÃ©-requisitos

- Python 3.11  
- Docker e Docker Compose (opcional, recomendado)  
- VS Code (recomendado)

### ğŸ”§ Ambiente local (sem Docker)

```bash
python -m venv .venv  
source .venv/bin/activate  # Windows: .venv\Scripts\activate  
pip install -r requirements.txt  
uvicorn api.main:app --reload

Acesse:

http://localhost:8000

http://localhost:8000/docs
```
ğŸ³ Com Docker Compose
```
docker compose up --build

Acesse:

API: http://localhost:8000

MLflow: http://localhost:5000
```

ğŸ“š Index RAG
Adicione documentos em data/docs/*.txt

Use RAGStore.build_or_load_index() e save_index() para construir o Ã­ndice

---

ğŸ§ª Testes / Tests

pytest -q

---

ğŸ§° Tecnologias / Technologies

<p align="left"> <img alt="Python" title="Python" width="50px" src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg"/>
<img alt="FastAPI" title="FastAPI" width="50px" src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/fastapi/fastapi-original.svg"/>
<img alt="Docker" title="Docker" width="50px" src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-original.svg"/>
<img alt="LangChain" title="LangChain" width="50px" src="https://avatars.githubusercontent.com/u/139903294?s=200&v=4"/>
<img alt="MLflow" title="MLflow" width="50px" src="https://raw.githubusercontent.com/mlflow/mlflow/master/assets/logo-white.svg"/>
<img alt="Hugging Face" title="Hugging Face" width="50px" src="https://huggingface.co/front/assets/huggingface_logo.svg"/> </p>

---

## ğŸ“œ LicenÃ§a / License

Este projeto estÃ¡ sob licenÃ§a MIT. Para mais detalhes, veja o arquivo `LICENSE`.  

This project is under the MIT license. For more details, see the `LICENSE` file.

---

ğŸ ConclusÃ£o / Conclusion

Este projeto representa uma aplicaÃ§Ã£o prÃ¡tica de LLMs e MLOps, integrando componentes modernos de IA para criar um agente inteligente capaz de responder com contexto, rastrear inferÃªncias e escalar via Docker/Kubernetes. Ideal para uso em sistemas de atendimento, assistentes virtuais ou plataformas de conhecimento interno.
